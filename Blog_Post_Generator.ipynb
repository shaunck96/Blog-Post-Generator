{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCCkyk16qvqEKxlO7YDjGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunck96/Blog-Post-Generator/blob/main/Blog_Post_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.2\n",
        "!pip install requests==2.31.0\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install openai==0.28\n",
        "!pip install eyed3==0.9.7\n",
        "!pip install tiktoken==0.5.1\n",
        "!pip install langchain==0.0.340\n",
        "!pip install langchain_community\n",
        "!pip install google"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhIrlWnXlwHm",
        "outputId": "477222be-feae-41f9-ae3a-07fbde4d60e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2023.11.17)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2023.11.17)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Collecting eyed3==0.9.7\n",
            "  Downloading eyed3-0.9.7-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coverage[toml]<6.0.0,>=5.3.1 (from eyed3==0.9.7)\n",
            "  Downloading coverage-5.5-cp310-cp310-manylinux1_x86_64.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0.0,>=2.1.0 (from eyed3==0.9.7)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.0.7 (from eyed3==0.9.7)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from coverage[toml]<6.0.0,>=5.3.1->eyed3==0.9.7) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->eyed3==0.9.7) (23.2)\n",
            "Installing collected packages: filetype, deprecation, coverage, eyed3\n",
            "Successfully installed coverage-5.5 deprecation-2.1.0 eyed3-0.9.7 filetype-1.2.0\n",
            "Collecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.79-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.79 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.3)\n",
            "Collecting langchain-core<0.2,>=0.1.8 (from langchain_community)\n",
            "  Downloading langchain_core-0.1.9-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.0.79)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.10.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2023.11.17)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.8->langchain_community) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain-core, langchain_community\n",
            "Successfully installed langchain-core-0.1.9 langchain_community-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "import openai\n",
        "import regex as re\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "import regex as re\n",
        "from transformers import pipeline\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib import parse\n",
        "import torch\n",
        "import ast\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from itertools import islice\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from itertools import islice\n",
        "import base64\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import googleapiclient.discovery\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain.llms import OpenAI\n",
        "import requests\n",
        "import requests\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "import nltk\n",
        "import time\n",
        "from typing import List\n",
        "from googlesearch import search\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvlXK1cklwFP",
        "outputId": "c2602d8d-3455-42cf-e8e9-b28ffad63ff3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Utility Functions\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-16k\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo-16k\") -> str:\n",
        "    \"\"\"\n",
        "    Query your LLM model with your prompt.\n",
        "    Parameters:\n",
        "    prompt (str): The text prompt you want the LLM to respond to.\n",
        "    model (str, optional): The model to be used for generating the response. Default is \"gpt-3.5-turbo\".\n",
        "    Returns:\n",
        "    str: The generated text completion from the specified model.\n",
        "    \"\"\"\n",
        "    openai.api_key = \"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\"\n",
        "    num_tokens = num_tokens_from_string(prompt, \"gpt-4\")\n",
        "    if num_tokens > 15000:\n",
        "      print(str(num_tokens)+\" :Too Long For GPT 4\")\n",
        "      return 0\n",
        "\n",
        "    else:\n",
        "      print(\"Number of tokens: \"+str(num_tokens))\n",
        "      input_cost = (int(num_tokens)/1000)*0.03\n",
        "      print(\"Cost Incurred Input: \"+str(input_cost))\n",
        "      messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "      start = time.time()\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model= model,\n",
        "          messages=messages,\n",
        "          temperature=0.5\n",
        "      )\n",
        "      output_cost = (int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06\n",
        "      print(\"Cost Incurred Output: \"+str((int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06))\n",
        "      return [response.choices[0].message[\"content\"], input_cost+output_cost, time.time()-start]\n",
        "\n",
        "def blog_post_inferencer(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class blog_post_details(BaseModel):\n",
        "      topics: list = Field(description=\"Main Topics Discussed in the Blog Post. Return atleast 3-4 distinct topics discussed\")\n",
        "\n",
        "  class blog(BaseModel):\n",
        "      blog: List[blog_post_details]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at evaluating a blog post. Extract the following key elements from the post :\n",
        "\n",
        "  topics\n",
        "\n",
        "  Blog Post to be evaluated:\n",
        "  {input_to_llm}\n",
        "\n",
        "  \"\"\"\n",
        "  pydantic_object=blog\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "def topic_info_generator(task='skill_extraction', input_to_llm='', context=\"\"):\n",
        "\n",
        "  class topic_info_details(BaseModel):\n",
        "      detailed_description: list = Field(description=\"Detailed Description of Topic in bullet points\")\n",
        "\n",
        "  class topic_info(BaseModel):\n",
        "      topic_info: List[topic_info_details]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at generating detailed descriptions for a particular topic using the blog post as context.\n",
        "\n",
        "  Blog Post To Reference Information:\n",
        "  {context}\n",
        "\n",
        "  Topic of interest:\n",
        "  {input_to_llm}\n",
        "\n",
        "  Stick to the above format. Only use the blog post to generate descriptions of the topic. Write the description in atleast 200 words. Stick to the bullet point format for the output.\n",
        "  \"\"\"\n",
        "  pydantic_object=topic_info\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "def topic_keyword_generator(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class topic_keywords(BaseModel):\n",
        "      topic_with_keywords: list = Field(description=\"Topic1: keywords, Topic2: keywords, ...\")\n",
        "\n",
        "  class topic_words(BaseModel):\n",
        "      topic_words: List[topic_keywords]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at generating keywords to search for the listed terms in google to get relevant content.\n",
        "\n",
        "  Topics Of Interest:\n",
        "  {', '.join(input_to_llm)}\n",
        "\n",
        "  Stick to the above format and return only topics with keywords.\n",
        "  \"\"\"\n",
        "  pydantic_object=topic_words\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "def url_info_extractor(urls):\n",
        "  data_dict = {}\n",
        "  for url in urls:\n",
        "    loader = WebBaseLoader(url)\n",
        "    try:\n",
        "      data = loader.load()\n",
        "      data_dict[url] = data\n",
        "    except:\n",
        "      print(\"Error\")\n",
        "      data_dict[url] = \"Unable to fetch data\"\n",
        "    else:\n",
        "      print(\"Success!\")\n",
        "  return data_dict\n",
        "\n",
        "def url_post_summarizer(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class url_info_extraction(BaseModel):\n",
        "      topic_and_description: list = Field(description=\"Topic 1: Description 1, Topic 2: Description 2,..\")\n",
        "\n",
        "  class url_info(BaseModel):\n",
        "      url_info: List[url_info_extraction]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at evaluating a blog post. Summarize the blog post highlighting key topics discussed with brief descriptions for each topic:\n",
        "\n",
        "  summary\n",
        "\n",
        "  Blog Post to be evaluated:\n",
        "  {input_to_llm}\n",
        "\n",
        "  Write the summary in atleast 200 words and at the most 500 words. Return only topics and descriptions in the output.\n",
        "  \"\"\"\n",
        "  pydantic_object=url_info_extraction\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "def url_info(content):\n",
        "  url_post_info_output = url_post_summarizer(input_to_llm=content.page_content)\n",
        "  #topics_and_desc = ast.literal_eval(url_post_info_output[0])['topic_and_description']#[0]\n",
        "  #return str(topics_and_desc)\n",
        "  if type(url_post_info_output) == int:\n",
        "    return \"Too Long For GPT 4\"\n",
        "  else:\n",
        "    print(url_post_info_output[0])\n",
        "    return url_post_info_output[0]"
      ],
      "metadata": {
        "id": "CHo74HcR3FrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://humanloop.com/blog/optimizing-llms\")\n",
        "data = loader.load()\n",
        "blog_details = blog_post_inferencer(input_to_llm=data)\n",
        "topics = ast.literal_eval(blog_details[0])['blog'][0]['topics']\n",
        "topic_description = {}\n",
        "\n",
        "for topic in topics:\n",
        "  blog_topics = topic_info_generator(input_to_llm=topic, context=data)\n",
        "  topic_description[topic] = ast.literal_eval(blog_topics[0])['topic_info'][0]['detailed_description'][0]\n",
        "\n",
        "topic_description = {}\n",
        "\n",
        "keywords = topic_keyword_generator(input_to_llm=topics)\n",
        "\n",
        "topic_keyword_dict = {}\n",
        "\n",
        "range(len(ast.literal_eval(keywords[0])['topic_words']))\n",
        "\n",
        "for index in range(len(ast.literal_eval(keywords[0])['topic_words'])):\n",
        "  topic_with_key = ast.literal_eval(keywords[0])['topic_words'][index]\n",
        "  topic = topic_with_key['topic_with_keywords'].split(':')[0]\n",
        "  key = topic_with_key['topic_with_keywords'].split(':')[1]\n",
        "  topic_keyword_dict[topic] = key\n",
        "\n",
        "url_df = pd.DataFrame(columns=['topic','url_list'])\n",
        "\n",
        "for topic in list(topic_keyword_dict.values()):\n",
        "  urls = []\n",
        "  query = topic.replace(\",\",\"\")+'blog'\n",
        "  for i in search (query,  tld='com', lang='en', tbs='0', safe='off', num=3, start=0, stop=10, domains=None, pause=2.0, tpe='', country='', extra_params=None, user_agent=None):\n",
        "    urls.append(i)\n",
        "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
        "\n",
        "url_df['url_with_content'] = url_df['url_list'].apply(url_info_extractor)\n",
        "\n",
        "for index in url_df.index:\n",
        "  online_context = \"\"\n",
        "  for content in list(url_df['url_with_content'].iloc[index].values()):\n",
        "    online_context+=url_info(content[0])\n",
        "    online_context+='\\n\\n'\n",
        "  url_df.at[index,'online_context'] = online_context\n",
        "\n",
        "url_df"
      ],
      "metadata": {
        "id": "wgjcPzhk7QJ2"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAhvKF8r2opK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}