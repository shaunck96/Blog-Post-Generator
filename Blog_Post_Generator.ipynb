{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj2rwQuCWCXRmSLLP8HXf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaunck96/Blog-Post-Generator/blob/main/Blog_Post_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.2\n",
        "!pip install requests==2.31.0\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install openai==0.28\n",
        "!pip install eyed3==0.9.7\n",
        "!pip install tiktoken==0.5.1\n",
        "!pip install langchain==0.0.340\n",
        "!pip install langchain_community\n",
        "!pip install google"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhIrlWnXlwHm",
        "outputId": "477222be-feae-41f9-ae3a-07fbde4d60e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2023.11.17)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2023.11.17)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Collecting eyed3==0.9.7\n",
            "  Downloading eyed3-0.9.7-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coverage[toml]<6.0.0,>=5.3.1 (from eyed3==0.9.7)\n",
            "  Downloading coverage-5.5-cp310-cp310-manylinux1_x86_64.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<3.0.0,>=2.1.0 (from eyed3==0.9.7)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.0.7 (from eyed3==0.9.7)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from coverage[toml]<6.0.0,>=5.3.1->eyed3==0.9.7) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->eyed3==0.9.7) (23.2)\n",
            "Installing collected packages: filetype, deprecation, coverage, eyed3\n",
            "Successfully installed coverage-5.5 deprecation-2.1.0 eyed3-0.9.7 filetype-1.2.0\n",
            "Collecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.1) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.79-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.79 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.3)\n",
            "Collecting langchain-core<0.2,>=0.1.8 (from langchain_community)\n",
            "  Downloading langchain_core-0.1.9-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.0.79)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.8->langchain_community) (1.10.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2023.11.17)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.8->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.8->langchain_community) (2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain-core, langchain_community\n",
            "Successfully installed langchain-core-0.1.9 langchain_community-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import regex as re\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from googleapiclient.discovery import build\n",
        "import regex as re\n",
        "from transformers import pipeline\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib import parse\n",
        "\n",
        "import torch\n",
        "import ast\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from itertools import islice\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from itertools import islice\n",
        "\n",
        "import base64\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import googleapiclient.discovery\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain.llms import OpenAI\n",
        "import requests\n",
        "import requests\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvlXK1cklwFP",
        "outputId": "c2602d8d-3455-42cf-e8e9-b28ffad63ff3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://huggingface.co/blog/optimize-llm\")"
      ],
      "metadata": {
        "id": "wgjcPzhk7QJ2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "epRORIHZ8XEo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPvqJ8ji8YP6",
        "outputId": "ae331009-ff37-48cb-ba00-cc3ff307fc2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOptimizing your LLM in production\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\tBack to blog\\n\\n\\n\\n\\n\\n\\t\\tOptimizing your LLM in production\\n\\t\\n\\nPublished\\n\\t\\t\\t\\tSeptember 15, 2023\\nUpdate on GitHub\\n\\npatrickvonplaten\\nPatrick von Platen\\n\\n\\n\\n\\n\\n\\n\\nNote: This blog post is also available as a documentation page on Transformers.\\nLarge Language Models (LLMs) such as GPT3/4, Falcon, and LLama are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.\\nDeploying these models in real-world tasks remains challenging, however:\\n\\nTo exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see Kaplan et al, Wei et. al). This consequently amplifies the memory demands for inference.\\nIn many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model\\'s capability to manage very long input sequences during inference.\\n\\nThe crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\\nIn this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:\\n\\nLower Precision: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.\\n\\nFlash Attention: Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\\n\\nArchitectural Innovations: Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).\\n\\n\\nThroughout this notebook, we will offer an analysis of auto-regressive generation from a tensor\\'s perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\\n\\n\\n\\n\\n\\n\\t\\t1. Harnessing the Power of Lower Precision\\n\\t\\n\\nMemory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition weights will be used to signify all model weight matrices and vectors.\\nAt the time of writing this post, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. 4.5689 which is usually stored in either float32, bfloat16, or float16 format. This allows us to easily compute the memory requirement to load the LLM into memory:\\n\\nLoading the weights of a model having X billion parameters requires roughly 4 * X GB of VRAM in float32 precision\\n\\nNowadays, models are however rarely trained in full float32 precision, but usually in bfloat16 precision or less frequently in float16 precision. Therefore the rule of thumb becomes:\\n\\nLoading the weights of a model having X billion parameters requires roughly 2 * X GB of VRAM in bfloat16/float16 precision\\n\\nFor shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let\\'s assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\\nTo give some examples of how much VRAM it roughly takes to load a model in bfloat16:\\n\\nGPT3 requires 2 * 175 GB = 350 GB VRAM\\nBloom requires 2 * 176 GB = 352 GB VRAM\\nLlama-2-70b requires 2 * 70 GB = 140 GB VRAM\\nFalcon-40b requires 2 * 40 GB = 80 GB VRAM\\nMPT-30b requires 2 * 30 GB = 60 GB VRAM\\nbigcode/starcoder requires 2 * 15.5 = 31 GB VRAM\\n\\nAs of writing this document, the largest GPU chip on the market is the A100 offering 80GB of VRAM. Most of the models listed before require more than 80GB just to be loaded and therefore necessarily require tensor parallelism and/or pipeline parallelism.\\n\\uf8ffü§ó Transformers does not support tensor parallelism out of the box as it requires the model architecture to be written in a specific way. If you\\'re interested in writing models in a tensor-parallelism-friendly way, feel free to have a look at the text-generation-inference library.\\nNaive pipeline parallelism is supported out of the box. For this, simply load the model with device=\"auto\" which will automatically place the different layers on the available GPUs as explained here.\\nNote, however that while very effective, this naive pipeline parallelism does not tackle the issues of GPU idling. For this more advanced pipeline parallelism is required as explained here.\\nIf you have access to an 8 x 80GB A100 node, you could load BLOOM as follows\\n!pip install transformers accelerate bitsandbytes optimum\\n\\nfrom transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"auto\", pad_token_id=0)\\n\\nBy using device_map=\"auto\" the attention layers would be equally distributed over all available GPUs.\\nIn this notebook, we will use bigcode/octocoder as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.\\nSince the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with bigcode/octocoder to be around 31 GB VRAM. Let\\'s give it a try.\\nWe first load the model and tokenizer and then pass both to Transformers\\' pipeline object.\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\nimport torch\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\\n\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\n\\nprompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\\\n\\\\nAnswer:\"\\n\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult\\n\\nOutput:\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n```python\\\\ndef bytes_to_giga_bytes(bytes):\\\\n    return bytes / 1024 / 1024 / 1024\\\\n```\\\\n\\\\nThis function takes a single\\n\\nNice, we can now directly use the result to convert bytes into Gigabytes.\\ndef bytes_to_giga_bytes(bytes):\\n  return bytes / 1024 / 1024 / 1024\\n\\nLet\\'s call torch.cuda.max_memory_allocated to measure the peak GPU memory allocation.\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n\\nOutput:\\n29.0260648727417\\n\\nClose enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000. Therefore the back-of-the-envelope formula can also be understood as an \"at most X GB\" computation.\\nNote that if we had tried to run the model in full float32 precision, a whopping 64 GB of VRAM would have been required.\\n\\nAlmost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if your GPU supports bfloat16. Float32 won\\'t give better inference results than the precision that was used to train the model.\\n\\nIf you are unsure in which format the model weights are stored on the Hub, you can always look into the checkpoint\\'s config under \"torch_dtype\", e.g. here. It is recommended to set the model to the same precision type as written in the config when loading with from_pretrained(..., torch_dtype=...) except when the original type is float32 in which case one can use both float16 or bfloat16 for inference.\\nLet\\'s define a flush(...) function to free all allocated memory so that we can accurately measure the peak allocated GPU memory.\\ndel pipe\\ndel model\\n\\nimport gc\\nimport torch\\n\\ndef flush():\\n  gc.collect()\\n  torch.cuda.empty_cache()\\n  torch.cuda.reset_peak_memory_stats()\\n\\nLet\\'s call it now for the next experiment.\\nflush()\\n\\nIn the recent version of the accelerate library, you can also use an utility method called release_memory()\\nfrom accelerate.utils import release_memory\\n# ...\\n\\nrelease_memory(model)\\n\\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see Dettmers et al.).\\nModel can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent GPTQ paper \\uf8ffü§Ø.\\nWithout going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model\\'s inference results as accurate as possible (a.k.a as close as possible to bfloat16).\\nNote that quantization works especially well for text generation since all we care about is choosing the set of most likely next tokens and don\\'t really care about the exact values of the next token logit distribution.\\nAll that matters is that the next token logit distribution stays roughly the same so that an argmax or topk operation gives the same results.\\nThere are various quantization techniques, which we won\\'t discuss in detail here, but in general, all quantization techniques work as follows:\\n\\n\\nQuantize all weights to the target precision\\n\\n\\n\\nLoad the quantized weights, and pass the input sequence of vectors in bfloat16 precision\\n\\n\\n\\nDynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision\\n\\n\\n\\nQuantize the weights again to the target precision after computation with their inputs.\\n\\n\\n\\nIn a nutshell, this means that inputs-weight matrix multiplications, with X X X being the inputs, W W W being a weight matrix and Y Y Y being the output:\\nY=X‚àóW Y = X * W Y=X‚àóW\\nare changed to\\nY=X‚àódequantize(W);quantize(W) Y = X * \\\\text{dequantize}(W); \\\\text{quantize}(W) Y=X‚àódequantize(W);quantize(W)\\nfor every matrix multiplication. Dequantization and re-quantization is performed sequentially for all weight matrices as the inputs run through the network graph.\\nTherefore, inference time is often not reduced when using quantized weights, but rather increases.\\nEnough theory, let\\'s give it a try! To quantize the weights with Transformers, you need to make sure that\\nthe bitsandbytes library is installed.\\n!pip install bitsandbytes\\n\\nWe can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\\n\\nNow, let\\'s run our example again and measure the memory usage.\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\n\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult\\n\\nOutput:\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n```python\\\\ndef bytes_to_giga_bytes(bytes):\\\\n    return bytes / 1024 / 1024 / 1024\\\\n```\\\\n\\\\nThis function takes a single\\n\\nNice, we\\'re getting the same result as before, so no loss in accuracy! Let\\'s look at how much memory was used this time.\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n\\nOutput:\\n15.219234466552734\\n\\nSignificantly less! We\\'re down to just a bit over 15 GBs and could therefore run this model on consumer GPUs like the 4090.\\nWe\\'re seeing a very nice gain in memory efficiency and more or less no degradation to the model\\'s output. However, we can also notice a slight slow-down during inference.\\nWe delete the models and flush the memory again.\\ndel model\\ndel pipe\\n\\nflush()\\n\\nLet\\'s see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing load_in_4bit=True instead of load_in_8bit=True.\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\\n\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\n\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult\\n\\nOutput:\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n```\\\\ndef bytes_to_gigabytes(bytes):\\\\n    return bytes / 1024 / 1024 / 1024\\\\n```\\\\n\\\\nThis function takes a single argument\\n\\nWe\\'re almost seeing the same output text as before - just the python is missing just before the code snippet. Let\\'s see how much memory was required.\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n\\nOutput:\\n9.543574333190918\\n\\nJust 9.5GB! That\\'s really not a lot for a >15 billion parameter model.\\nWhile we see very little degradation in accuracy for our model here, 4-bit quantization can in practice often lead to different results compared to 8-bit quantization or full bfloat16 inference. It is up to the user to try it out.\\nAlso note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to quantize \\\\text{quantize} quantize and dequantize \\\\text{dequantize} dequantize taking longer during inference.\\ndel model\\ndel pipe\\n\\nflush()\\n\\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32G GPU VRAM to only 15GB and running the model in 4-bit precision further reduces the required GPU VRAM to just a bit over 9GB.\\n4-bit quantization allows the model to be run on GPUs such as RTX3090, V100, and T4 which are quite accessible for most people.\\nFor more information on quantization and to see how one can quantize models to require even less GPU VRAM memory than 4-bit, we recommend looking into the AutoGPTQ implementation.\\n\\nAs a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.\\n\\nIf GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can\\'t run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.\\nFor more in-detail usage information, we strongly recommend taking a look at the Transformers Quantization Docs.\\nNext, let\\'s look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.\\n\\n\\n\\n\\n\\n\\t\\t2. Flash Attention: A Leap Forward\\n\\t\\n\\nToday\\'s top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.\\nSelf-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens.\\nHowever, the peak GPU memory consumption for self-attention layers grows quadratically both in compute and memory complexity with number of input tokens (also called sequence length) that we denote in the following by N N N .\\nWhile this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens).\\nLet\\'s take a closer look. The formula to compute the output O \\\\mathbf{O} O of a self-attention layer for an input X \\\\mathbf{X} X of length N N N is:\\nO=Attn(X)=V√óSoftmax(QKT)¬†with¬†Q=WqX,V=WvX,K=WkX \\\\textbf{O} = \\\\text{Attn}(\\\\mathbf{X}) = \\\\mathbf{V} \\\\times \\\\text{Softmax}(\\\\mathbf{QK}^T) \\\\text{ with } \\\\mathbf{Q} = \\\\mathbf{W}_q \\\\mathbf{X}, \\\\mathbf{V} = \\\\mathbf{W}_v \\\\mathbf{X}, \\\\mathbf{K} = \\\\mathbf{W}_k \\\\mathbf{X} O=Attn(X)=V√óSoftmax(QKT)¬†with¬†Q=Wq‚ÄãX,V=Wv‚ÄãX,K=Wk‚ÄãX\\nX=(x1,...xN)  \\\\mathbf{X} = (\\\\mathbf{x}_1, ... \\\\mathbf{x}_{N}) X=(x1‚Äã,...xN‚Äã) is thereby the input sequence to the attention layer. The projections Q \\\\mathbf{Q} Q and K \\\\mathbf{K} K will each consist of N N N vectors resulting in the QKT \\\\mathbf{QK}^T QKT being of size N2 N^2 N2 .\\nLLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel.\\nAssuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the QKT \\\\mathbf{QK^T} QKT matrices to be 40‚àó2‚àóN2 40 * 2 * N^2 40‚àó2‚àóN2 bytes. For N=1000 N=1000 N=1000 only around 50 MB of VRAM are needed, however, for N=16000 N=16000 N=16000 we would need 19 GB of VRAM, and for N=100,000 N=100,000 N=100,000 we would need almost 1TB just to store the QKT \\\\mathbf{QK}^T QKT matrices.\\nLong story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts.\\nAs LLMs improve in text comprehension and generation, they are applied to increasingly complex tasks. While models once handled the translation or summarization of a few sentences, they now manage entire pages, demanding the capability to process extensive input lengths.\\nHow can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the QKT QK^T QKT matrix. Tri Dao et al. developed exactly such a new algorithm and called it Flash Attention.\\nIn a nutshell, Flash Attention breaks the  V√óSoftmax(QKT\\\\mathbf{V} \\\\times \\\\text{Softmax}(\\\\mathbf{QK}^TV√óSoftmax(QKT) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:\\nOi‚Üêsija‚àóOi+sijb‚àóVj√óSoftmax(QKi,jT)¬†for¬†multiple¬†i,j¬†iterations \\\\textbf{O}_i \\\\leftarrow s^a_{ij} * \\\\textbf{O}_i + s^b_{ij} * \\\\mathbf{V}_{j} \\\\times \\\\text{Softmax}(\\\\mathbf{QK}^T_{i,j}) \\\\text{ for multiple } i, j \\\\text{ iterations} Oi‚Äã‚Üêsija‚Äã‚àóOi‚Äã+sijb‚Äã‚àóVj‚Äã√óSoftmax(QKi,jT‚Äã)¬†for¬†multiple¬†i,j¬†iterations\\nwith sija s^a_{ij} sija‚Äã and sijb s^b_{ij} sijb‚Äã being some softmax normalization statistics that need to be recomputed for every i i i and j j j .\\nPlease note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this notebook. The reader is invited to take a look at the well-written Flash Attention paper for more details.\\nThe main takeaway here is:\\n\\nBy keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives numerical identical outputs compared to the default self-attention layer at a memory cost that only increases linearly with N N N .\\n\\nLooking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see paper for more details if interested)\\n\\nHowever, Flash Attention is much faster in inference compared to default attention which comes from its ability to significantly reduce the demands on the slower, high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip memory (SRAM).\\n\\nEssentially, Flash Attention makes sure that all intermediate write and read operations can be done using the fast on-chip SRAM memory instead of having to access the slower VRAM memory to compute the output vector O \\\\mathbf{O} O .\\nIn practice, there is currently absolutely no reason to not use Flash Attention if available. The algorithm gives mathematically the same outputs, and is both faster and more memory-efficient.\\nLet\\'s look at a practical example.\\nOur OctoCoder model now gets a significantly longer input prompt which includes a so-called system prompt. System prompts are used to steer the LLM into a better assistant that is tailored to the users\\' task.\\nIn the following, we use a system prompt that will make OctoCoder a better coding assistant.\\nsystem_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\\nThe assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\\nThe assistant is happy to help with code questions and will do their best to understand exactly what is needed.\\nIt also tries to avoid giving false or misleading information, and it caveats when it isn\\'t entirely sure about the right answer.\\nThat said, the assistant is practical really does its best, and doesn\\'t let caution get too much in the way of being useful.\\n\\nThe Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\\nThe model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\\n\\n-----\\n\\nQuestion: Write a function that takes two lists and returns a list that has alternating elements from each input list.\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef alternating(list1, list2):\\n   results = []\\n   for i in range(len(list1)):\\n       results.append(list1[i])\\n       results.append(list2[i])\\n   return results\\n\\nQuestion: Can you write some test cases for this function?\\n\\nAnswer: Sure, here are some tests.\\n\\nassert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\\nassert alternating([True, False], [4, 5]) == [True, 4, False, 5]\\nassert alternating([], []) == []\\n\\nQuestion: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\\n\\nAnswer: Here is the modified function.\\n\\ndef alternating(list1, list2):\\n   results = []\\n   for i in range(min(len(list1), len(list2))):\\n       results.append(list1[i])\\n       results.append(list2[i])\\n   if len(list1) > len(list2):\\n       results.extend(list1[i+1:])\\n   else:\\n       results.extend(list2[i+1:])\\n   return results\\n\\n-----\\n\"\"\"\\n\\nFor demonstration purposes, we duplicate the system by ten so that the input length is long enough to observe Flash Attention\\'s memory savings.\\nWe append the original text prompt \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\\\n\\\\nAnswer: Here\"\\nlong_prompt = 10 * system_prompt + prompt\\n\\nWe instantiate our model again in bfloat16 precision.\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\\n\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\n\\nLet\\'s now run the model just like before without Flash Attention and measure the peak GPU memory requirement and inference time.\\nimport time\\n\\nstart_time = time.time()\\nresult = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\\n\\nprint(f\"Generated in {time.time() - start_time} seconds.\")\\nresult\\n\\nOutput:\\nGenerated in 10.96854019165039 seconds.\\nSure. Here is a function that does that.\\\\n\\\\ndef bytes_to_giga(bytes):\\\\n   return bytes / 1024 / 1024 / 1024\\\\n\\\\nAnswer: Sure. Here is a function that does that.\\\\n\\\\ndef\\n\\nWe\\'re getting the same output as before, however this time, the model repeats the answer multiple times until it\\'s 60 tokens cut-off. This is not surprising as we\\'ve repeated the system prompt ten times for demonstration purposes and thus cued the model to repeat itself.\\nNote that the system prompt should not be repeated ten times in real-world applications - one time is enough!\\nLet\\'s measure the peak GPU memory requirement.\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n\\nOutput:\\n37.668193340301514\\n\\nAs we can see the peak GPU memory requirement is now significantly higher than in the beginning, which is largely due to the longer input sequence. Also the generation takes a little over a minute now.\\nWe call flush() to free GPU memory for our next experiment.\\nflush()\\n\\nFor comparison, let\\'s run the same function, but enable Flash Attention instead.\\nTo do so, we convert the model to BetterTransformers and by doing so enabling PyTorch\\'s SDPA self-attention which in turn is based on Flash Attention.\\nmodel.to_bettertransformer()\\n\\nNow we run the exact same code snippet as before and under the hood Transformers will make use of Flash Attention.\\nstart_time = time.time()\\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\\n    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\\n\\nprint(f\"Generated in {time.time() - start_time} seconds.\")\\nresult\\n\\nOutput:\\nGenerated in 3.0211617946624756 seconds.\\n Sure. Here is a function that does that.\\\\n\\\\ndef bytes_to_giga(bytes):\\\\n   return bytes / 1024 / 1024 / 1024\\\\n\\\\nAnswer: Sure. Here is a function that does that.\\\\n\\\\ndef\\n\\nWe\\'re getting the exact same result as before, but can observe a very significant speed-up thanks to Flash Attention.\\nLet\\'s measure the memory consumption one last time.\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\n\\nOutput:\\n32.617331981658936\\n\\nAnd we\\'re almost back to our original 29GB peak GPU memory from the beginning.\\nWe can observe that we only use roughly 100MB more GPU memory when passing a very long input sequence with Flash Attention compared to passing a short input sequence as done in the beginning.\\nflush()\\n\\n\\n\\n\\n\\n\\n\\t\\t3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat\\n\\t\\n\\nSo far we have looked into improving computational and memory efficiency by:\\n\\nCasting the weights to a lower precision format\\nReplacing the self-attention algorithm with a more memory- and compute efficient version\\n\\nLet\\'s now look into how we can change the architecture of an LLM so that it is most effective and efficient for task that require long text inputs, e.g.:\\n\\nRetrieval augmented Questions Answering,\\nSummarization,\\nChat\\n\\nNote that chat not only requires the LLM to handle long text inputs, but it also necessitates that the LLM is able to efficiently handle the back-and-forth dialogue between user and assistant (such as ChatGPT).\\nOnce trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM\\'s tasks beforehand and accordingly optimize the model\\'s architecture.\\nThere are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.\\n\\nThe positional embeddings\\nThe key-value cache\\n\\nLet\\'s go over each component in more detail\\n\\n\\n\\n\\n\\n\\t\\t3.1 Improving positional embeddings of LLMs\\n\\t\\n\\nSelf-attention puts each token in relation to each other\\'s tokens.\\nAs an example, the Softmax(QKT) \\\\text{Softmax}(\\\\mathbf{QK}^T) Softmax(QKT) matrix of the text input sequence \"Hello\", \"I\", \"love\", \"you\" could look as follows:\\n\\nEach word token is given a probability mass at which it attends all other word tokens and, therefore is put into relation with all other word tokens. E.g. the word \"love\" attends to the word \"Hello\" with 5%, to \"I\" with 30%, and to itself with 65%.\\nA LLM based on self-attention, but without position embeddings would have great difficulties in understanding the positions of the text inputs to each other.\\nThis is because the probability score computed by QKT \\\\mathbf{QK}^T QKT relates each word token to each other word token in O(1) O(1) O(1) computations regardless of their relative positional distance to each other.\\nTherefore, for the LLM without position embeddings each token appears to have the same distance to all other tokens, e.g. differentiating between \"Hello I love you\" and \"You love I hello\" would be very challenging.\\nFor the LLM to understand sentence order, an additional cue is needed and is usually applied in the form of positional encodings (or also called positional embeddings).\\nPositional encodings, encode the position of each token into a numerical presentation that the LLM can leverage to better understand sentence order.\\nThe authors of the Attention Is All You Need paper introduced sinusoidal positional embeddings P=p1,‚Ä¶,pN \\\\mathbf{P} = \\\\mathbf{p}_1, \\\\ldots, \\\\mathbf{p}_N P=p1‚Äã,‚Ä¶,pN‚Äã .\\nwhere each vector pi \\\\mathbf{p}_i pi‚Äã is computed as a sinusoidal function of its position i i i .\\nThe positional encodings are then simply added to the input sequence vectors X^=x^1,‚Ä¶,x^N \\\\mathbf{\\\\hat{X}} = \\\\mathbf{\\\\hat{x}}_1, \\\\ldots, \\\\mathbf{\\\\hat{x}}_N X^=x^1‚Äã,‚Ä¶,x^N‚Äã = x1+p1,‚Ä¶,xN+pN \\\\mathbf{x}_1 + \\\\mathbf{p}_1, \\\\ldots, \\\\mathbf{x}_N + \\\\mathbf{p}_N x1‚Äã+p1‚Äã,‚Ä¶,xN‚Äã+pN‚Äã thereby cueing the model to better learn sentence order.\\nInstead of using fixed position embeddings, others (such as Devlin et al.) used learned positional encodings for which the positional embeddings P \\\\mathbf{P} P are learned during training.\\nSinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:\\n\\nSinusoidal and learned position embeddings are both absolute positional embeddings, i.e. encoding a unique embedding for each position id: 0,‚Ä¶,N 0, \\\\ldots, N 0,‚Ä¶,N . As shown by Huang et al. and Su et al., absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.\\nWhen using learned position embeddings, the LLM has to be trained on a fixed input length N N N, which makes it difficult to extrapolate to an input length longer than what it was trained on.\\n\\nRecently, relative positional embeddings that can tackle the above mentioned problems have become more popular, most notably:\\n\\nRotary Position Embedding (RoPE)\\nALiBi\\n\\nBoth RoPE and ALiBi argue that it\\'s best to cue the LLM about sentence order directly in the self-attention algorithm as it\\'s there that word tokens are put into relation with each other. More specifically, sentence order should be cued by modifying the QKT \\\\mathbf{QK}^T QKT computation.\\nWithout going into too many details, RoPE notes that positional information can be encoded into query-key pairs, e.g. qi \\\\mathbf{q}_i qi‚Äã and xj \\\\mathbf{x}_j xj‚Äã by rotating each vector by an angle Œ∏‚àói \\\\theta * i Œ∏‚àói and Œ∏‚àój \\\\theta * j Œ∏‚àój respectively with i,j i, j i,j describing each vectors sentence position:\\nq^iTx^j=qiTRŒ∏,i‚àíjxj. \\\\mathbf{\\\\hat{q}}_i^T \\\\mathbf{\\\\hat{x}}_j = \\\\mathbf{{q}}_i^T \\\\mathbf{R}_{\\\\theta, i -j} \\\\mathbf{{x}}_j. q^‚ÄãiT‚Äãx^j‚Äã=qiT‚ÄãRŒ∏,i‚àíj‚Äãxj‚Äã.\\nRŒ∏,i‚àíj \\\\mathbf{R}_{\\\\theta, i - j} RŒ∏,i‚àíj‚Äã thereby represents a rotational matrix. Œ∏ \\\\theta Œ∏ is not learned during training, but instead set to a pre-defined value that depends on the maximum input sequence length during training.\\n\\nBy doing so, the propability score between qi \\\\mathbf{q}_i qi‚Äã and qj \\\\mathbf{q}_j qj‚Äã is only affected if i‚â†j i \\\\ne j iÓÄ†=j and solely depends on the relative distance i‚àíj i - j i‚àíj regardless of each vector\\'s specific positions i i i and j j j .\\n\\nRoPE is used in multiple of today\\'s most important LLMs, such as:\\n\\nFalcon\\nLlama\\nPaLM\\n\\nAs an alternative, ALiBi proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value m to each query-key entry of the QKT \\\\mathbf{QK}^T QKT matrix right before the softmax computation.\\n\\nAs shown in the ALiBi paper, this simple relative positional encoding allows the model to retain a high performance even at very long text input sequences.\\nALiBi is used in multiple of today\\'s most important LLMs, such as:\\n\\nMPT\\nBLOOM\\n\\nBoth RoPE and ALiBi position encodings can extrapolate to input lengths not seen during training whereas it has been shown that extrapolation works much better out-of-the-box for ALiBi as compared to RoPE.\\nFor ALiBi, one simply increases the values of the lower triangular position matrix to match the length of the input sequence.\\nFor RoPE, keeping the same Œ∏ \\\\theta Œ∏ that was used during training leads to poor results when passing text inputs much longer than those seen during training, c.f Press et al.. However, the community has found a couple of effective tricks that adapt Œ∏ \\\\theta Œ∏, thereby allowing RoPE position embeddings to work well for extrapolated text input sequences (see here).\\n\\nBoth RoPE and ALiBi are relative positional embeddings that are not learned during training, but instead are based on the following intuitions:\\n\\n\\nPositional cues about the text inputs should be given directly to the QKT QK^T QKT matrix of the self-attention layer\\nThe LLM should be incentivized to learn a constant relative distance positional encodings have to each other\\nThe further text input tokens are from each other, the lower the probability of their query-value probability. Both RoPE and ALiBi lower the query-key probability of tokens far away from each other. RoPE by decreasing their vector product by increasing the angle between the query-key vectors. ALiBi by adding large negative numbers to the vector product\\n\\nIn conclusion, LLMs that are intended to be deployed in tasks that require handling large text inputs are better trained with relative positional embeddings, such as RoPE and ALiBi. Also note that even if an LLM with RoPE and ALiBi has been trained only on a fixed length of say N1=2048 N_1 = 2048 N1‚Äã=2048 it can still be used in practice with text inputs much larger than N1 N_1 N1‚Äã, like N2=8192>N1 N_2 = 8192 > N_1 N2‚Äã=8192>N1‚Äã by extrapolating the positional embeddings.\\n\\n\\n\\n\\n\\n\\t\\t3.2 The key-value cache\\n\\t\\n\\nAuto-regressive text generation with LLMs works by iteratively putting in an input sequence, sampling the next token, appending the next token to the input sequence, and continuing to do so until the LLM produces a token that signifies that the generation has finished.\\nPlease have a look at Transformer\\'s Generate Text Tutorial to get a more visual explanation of how auto-regressive generation works.\\nLet\\'s run a quick code snippet to show how auto-regressive works in practice. We will simply take the most likely next token via torch.argmax.\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\\n\\nfor _ in range(5):\\n  next_logits = model(input_ids)[\"logits\"][:, -1:]\\n  next_token_id = torch.argmax(next_logits,dim=-1)\\n\\n  input_ids = torch.cat([input_ids, next_token_id], dim=-1)\\n  print(\"shape of input_ids\", input_ids.shape)\\n\\ngenerated_text = tokenizer.batch_decode(input_ids[:, -5:])\\ngenerated_text\\n\\nOutput:\\nshape of input_ids torch.Size([1, 21])\\nshape of input_ids torch.Size([1, 22])\\nshape of input_ids torch.Size([1, 23])\\nshape of input_ids torch.Size([1, 24])\\nshape of input_ids torch.Size([1, 25])\\n[\\' Here is a Python function\\']\\n\\nAs we can see every time we increase the text input tokens by the just sampled token.\\nWith very few exceptions, LLMs are trained using the causal language modeling objective and therefore mask the upper triangle matrix of the attention score - this is why in the two diagrams above the attention scores are left blank (a.k.a have 0 probability). For a quick recap on causal language modeling you can refer to the Illustrated Self Attention blog.\\nAs a consequence, tokens never depend on previous tokens, more specifically the qi \\\\mathbf{q}_i qi‚Äã vector is never put in relation with any key, values vectors kj,vj \\\\mathbf{k}_j, \\\\mathbf{v}_j kj‚Äã,vj‚Äã if j>i j > i j>i . Instead qi \\\\mathbf{q}_i qi‚Äã only attends to previous key-value vectors km<i,vm<i¬†,¬†for¬†m‚àà{0,‚Ä¶i‚àí1} \\\\mathbf{k}_{m < i}, \\\\mathbf{v}_{m < i} \\\\text{ , for } m \\\\in \\\\{0, \\\\ldots i - 1\\\\} km<i‚Äã,vm<i‚Äã¬†,¬†for¬†m‚àà{0,‚Ä¶i‚àí1}. In order to reduce unnecessary computation, one can therefore cache each layer\\'s key-value vectors for all previous timesteps.\\nIn the following, we will tell the LLM to make use of the key-value cache by retrieving and forwarding it for each forward pass.\\nIn Transformers, we can retrieve the key-value cache by passing the use_cache flag to the forward call and can then pass it with the current token.\\npast_key_values = None # past_key_values is the key-value cache\\ngenerated_tokens = []\\nnext_token_id = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\\n\\nfor _ in range(5):\\n  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\\n  next_logits = next_logits[:, -1:]\\n  next_token_id = torch.argmax(next_logits, dim=-1)\\n\\n  print(\"shape of input_ids\", next_token_id.shape)\\n  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\\n  generated_tokens.append(next_token_id.item())\\n\\ngenerated_text = tokenizer.batch_decode(generated_tokens)\\ngenerated_text\\n\\nOutput:\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 20\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 21\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 22\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 23\\nshape of input_ids torch.Size([1, 1])\\nlength of key-value cache 24\\n[\\' Here\\', \\' is\\', \\' a\\', \\' Python\\', \\' function\\']\\n\\nAs one can see, when using the key-value cache the text input tokens are not increased in length, but remain a single input vector. The length of the key-value cache on the other hand is increased by one at every decoding step.\\n\\nMaking use of the key-value cache means that the QKT \\\\mathbf{QK}^T QKT is essentially reduced to qcKT \\\\mathbf{q}_c\\\\mathbf{K}^T qc‚ÄãKT with qc \\\\mathbf{q}_c qc‚Äã being the query projection of the currently passed input token which is always just a single vector.\\n\\nUsing the key-value cache has two advantages:\\n\\nSignificant increase in computational efficiency as less computations are performed compared to computing the full QKT \\\\mathbf{QK}^T QKT matrix. This leads to an increase in inference speed\\nThe maximum required memory is not increased quadratically with the number of generated tokens, but only increases linearly.\\n\\n\\nOne should always make use of the key-value cache as it leads to identical results and a significant speed-up for longer input sequences. Transformers has the key-value cache enabled by default when making use of the text pipeline or the generate method.\\n\\nNote that the key-value cache is especially useful for applications such as chat where multiple passes of auto-regressive decoding are required. Let\\'s look at an example.\\nUser: How many people live in France?\\nAssistant: Roughly 75 million people live in France\\nUser: And how many are in Germany?\\nAssistant: Germany has ca. 81 million inhabitants\\n\\nIn this chat, the LLM runs auto-regressive decoding twice:\\n\\n\\nThe first time, the key-value cache is empty and the input prompt is \"User: How many people live in France?\" and the model auto-regressively generates the text \"Roughly 75 million people live in France\" while increasing the key-value cache at every decoding step.\\n\\n\\n\\nThe second time the input prompt is \"User: How many people live in France? \\\\n Assistant: Roughly 75 million people live in France \\\\n User: And how many in Germany?\". Thanks to the cache, all key-value vectors for the first two sentences are already computed. Therefore the input prompt only consists of \"User: And how many in Germany?\". While processing the shortened input prompt, it\\'s computed key-value vectors are concatenated to the key-value cache of the first decoding. The second Assistant\\'s answer \"Germany has ca. 81 million inhabitants\" is then auto-regressively generated with the key-value cache consisting of encoded key-value vectors of \"User: How many people live in France? \\\\n Assistant: Roughly 75 million people live in France \\\\n User: And how many are in Germany?\".\\n\\n\\n\\nTwo things should be noted here:\\n\\nKeeping all the context is crucial for LLMs deployed in chat so that the LLM understands all the previous context of the conversation. E.g. for the example above the LLM needs to understand that the user refers to the population when asking \"And how many are in Germany\".\\nThe key-value cache is extremely useful for chat as it allows us to continuously grow the encoded chat history instead of having to re-encode the chat history again from scratch (as e.g. would be the case when using an encoder-decoder architecture).\\n\\nThere is however one catch. While the required peak memory for the QKT \\\\mathbf{QK}^T QKT matrix is significantly reduced, holding the key-value cache in memory can become very memory expensive for long input sequences or multi-turn chat. Remember that the key-value cache needs to store the key-value vectors for all previous input vectors xi,¬†for¬†i‚àà{1,‚Ä¶,c‚àí1} \\\\mathbf{x}_i \\\\text{, for } i \\\\in \\\\{1, \\\\ldots, c - 1\\\\} xi‚Äã,¬†for¬†i‚àà{1,‚Ä¶,c‚àí1} for all self-attention layers and for all attention heads.\\nLet\\'s compute the number of float values that need to be stored in the key-value cache for the LLM bigcode/octocoder that we used before.\\nThe number of float values amounts to two times the sequence length times the number of attention heads times the attention head dimension and times the number of layers.\\nComputing this for our LLM at a hypothetical input sequence length of 16000 gives:\\nconfig = model.config\\n2 * 16_000 * config.n_layer * config.n_head * config.n_embd // config.n_head\\n\\nOutput:\\n7864320000\\n\\nRoughly 8 billion float values! Storing 8 billion float values in float16 precision requires around 15 GB of RAM which is circa half as much as the model weights themselves!\\nResearchers have proposed two methods that allow to significantly reduce the memory cost of storing the key-value cache:\\n\\nMulti-Query-Attention (MQA)\\n\\nMulti-Query-Attention was proposed in Noam Shazeer\\'s Fast Transformer Decoding: One Write-Head is All You Need paper. As the title says, Noam found out that instead of using n_head key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model\\'s performance significantly degrades.\\n\\nBy using a single head-value projection weight pair, the key value vectors ki,vi \\\\mathbf{k}_i, \\\\mathbf{v}_i ki‚Äã,vi‚Äã have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of n_head ones.\\n\\nAs most LLMs use between 20 and 100 attention heads, MQA significantly reduces the memory consumption of the key-value cache. For the LLM used in this notebook we could therefore reduce the required memory consumption from 15 GB to less than 400 MB at an input sequence length of 16000.\\nIn addition to memory savings, MQA also leads to improved computational efficiency as explained in the following.\\nIn auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the qcKT \\\\mathbf{q}_c\\\\mathbf{K}^T qc‚ÄãKT computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at Noam\\'s paper.\\nThe important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different QKT \\\\mathbf{QK}^T QKT matrix.\\nMQA has seen wide adoption by the community and is now used by many of the most popular LLMs:\\n\\nFalcon\\nPaLM\\nMPT\\nBLOOM\\n\\nAlso, the checkpoint used in this notebook - bigcode/octocoder - makes use of MQA.\\n\\nGrouped-Query-Attention (GQA)\\n\\nGrouped-Query-Attention, as proposed by Ainslie et al. from Google, found that using MQA can often lead to quality degradation compared to using vanilla multi-key-value head projections. The paper argues that more model performance can be kept by less drastically reducing the number of query head projection weights. Instead of using just a single key-value projection weight, n < n_head key-value projection weights should be used. By choosing n to a significantly smaller value than n_head, such as 2,4 or 8 almost all of the memory and speed gains from MQA can be kept while sacrificing less model capacity and thus arguably less performance.\\nMoreover, the authors of GQA found out that existing model checkpoints can be uptrained to have a GQA architecture with as little as 5% of the original pre-training compute. While 5% of the original pre-training compute can still be a massive amount, GQA uptraining allows existing checkpoints to be useful for longer input sequences.\\nGQA was only recently proposed which is why there is less adoption at the time of writing this notebook.\\nThe most notable application of GQA is Llama-v2.\\n\\nAs a conclusion, it is strongly recommended to make use of either GQA or MQA if the LLM is deployed with auto-regressive decoding and is required to handle large input sequences as is the case for example for chat.\\n\\n\\n\\n\\n\\n\\n\\t\\tConclusion\\n\\t\\n\\nThe research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is speculative decoding where \"easy tokens\" are generated by smaller, faster language models and only \"hard tokens\" are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this nice blog post.\\nThe reason massive LLMs such as GPT3/4, Llama-2-70b, Claude, PaLM can run so quickly in chat-interfaces such as Hugging Face Chat or ChatGPT is to a big part thanks to the above-mentioned improvements in precision, algorithms, and architecture.\\nGoing forward, accelerators such as GPUs, TPUs, etc... will only get faster and allow for more memory, but one should nevertheless always make sure to use the best available algorithms and architectures to get the most bang for your buck \\uf8ffü§ó\\n\\nMore articles from our Blog\\n\\n\\n2023, year of open LLMs\\n\\t\\t\\t\\nBy¬†\\nclefourrier\\n\\n\\nDecember 18, 2023\\n\\n\\n\\nWelcome Mixtral - a SOTA Mixture of Experts on Hugging Face\\n\\t\\t\\t\\nBy¬†\\nlewtun\\n\\n\\nDecember 11, 2023\\n\\n\\nCompany\\n¬© Hugging Face\\nTOS\\nPrivacy\\nAbout\\nJobs\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://huggingface.co/blog/optimize-llm', 'title': 'Optimizing your LLM in production', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'})]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "o3rC-ZL2nLvU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-16k\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo-16k\") -> str:\n",
        "    \"\"\"\n",
        "    Query your LLM model with your prompt.\n",
        "    Parameters:\n",
        "    prompt (str): The text prompt you want the LLM to respond to.\n",
        "    model (str, optional): The model to be used for generating the response. Default is \"gpt-3.5-turbo\".\n",
        "    Returns:\n",
        "    str: The generated text completion from the specified model.\n",
        "    \"\"\"\n",
        "    openai.api_key = \"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\"\n",
        "    num_tokens = num_tokens_from_string(prompt, \"gpt-4\")\n",
        "    if num_tokens > 15000:\n",
        "      print(str(num_tokens)+\" :Too Long For GPT 4\")\n",
        "      return 0\n",
        "\n",
        "    else:\n",
        "      print(\"Number of tokens: \"+str(num_tokens))\n",
        "      input_cost = (int(num_tokens)/1000)*0.03\n",
        "      print(\"Cost Incurred Input: \"+str(input_cost))\n",
        "      messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "      start = time.time()\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model= model,\n",
        "          messages=messages,\n",
        "          temperature=0.5\n",
        "      )\n",
        "      output_cost = (int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06\n",
        "      print(\"Cost Incurred Output: \"+str((int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06))\n",
        "      return [response.choices[0].message[\"content\"], input_cost+output_cost, time.time()-start]\n",
        "\n",
        "def blog_post_inferencer(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class blog_post_details(BaseModel):\n",
        "      topics: list = Field(description=\"Main Topics Discussed in the Blog Post\")\n",
        "\n",
        "  class blog(BaseModel):\n",
        "      blog: List[blog_post_details]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at evaluating a blog post. Extract the following key elements from the post :\n",
        "\n",
        "  topics\n",
        "\n",
        "  Blog Post to be evaluated:\n",
        "  {input_to_llm}\n",
        "\n",
        "  \"\"\"\n",
        "  pydantic_object=blog\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "blog_details = blog_post_inferencer(input_to_llm=data)"
      ],
      "metadata": {
        "id": "14_09hSAlqnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db599a81-d602-404d-e804-3330ff3f36c2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 12370\n",
            "Cost Incurred Input: 0.3711\n",
            "Cost Incurred Output: 0.00606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blog_topic_list = ast.literal_eval(blog_details[0])['blog']\n",
        "topics = []\n",
        "\n",
        "for i in range(len(blog_topic_list)):\n",
        "  topics.append(blog_topic_list[i]['topics'][0])\n",
        "\n",
        "topics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frK3XMHv9uS7",
        "outputId": "a0c53812-3fe8-4f30-bd07-ab7b5d8ad1e0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Optimizing your LLM in production',\n",
              " 'Lower Precision',\n",
              " 'Flash Attention',\n",
              " 'Architectural Innovations',\n",
              " 'Multi-Query-Attention (MQA)',\n",
              " 'Grouped-Query-Attention (GQA)',\n",
              " 'Positional embeddings',\n",
              " 'The key-value cache',\n",
              " 'Conclusion']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo-16k\") -> str:\n",
        "    \"\"\"\n",
        "    Query your LLM model with your prompt.\n",
        "    Parameters:\n",
        "    prompt (str): The text prompt you want the LLM to respond to.\n",
        "    model (str, optional): The model to be used for generating the response. Default is \"gpt-3.5-turbo\".\n",
        "    Returns:\n",
        "    str: The generated text completion from the specified model.\n",
        "    \"\"\"\n",
        "    openai.api_key = \"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\"\n",
        "    num_tokens = num_tokens_from_string(prompt, \"gpt-4\")\n",
        "    if num_tokens > 15000:\n",
        "      print(str(num_tokens)+\" :Too Long For GPT 4\")\n",
        "      return 0\n",
        "\n",
        "    else:\n",
        "      print(\"Number of tokens: \"+str(num_tokens))\n",
        "      input_cost = (int(num_tokens)/1000)*0.03\n",
        "      print(\"Cost Incurred Input: \"+str(input_cost))\n",
        "      messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "      start = time.time()\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model= model,\n",
        "          messages=messages,\n",
        "          temperature=0.5\n",
        "      )\n",
        "      output_cost = (int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06\n",
        "      print(\"Cost Incurred Output: \"+str((int(num_tokens_from_string(response.choices[0].message[\"content\"], \"gpt-4\"))/1000)*0.06))\n",
        "      return [response.choices[0].message[\"content\"], input_cost+output_cost, time.time()-start]\n",
        "\n",
        "def topic_info_generator(task='skill_extraction', input_to_llm='', context=\"\"):\n",
        "\n",
        "  class topic_info_details(BaseModel):\n",
        "      detailed_description: list = Field(description=\"Detailed Description of Topic in bullet points\")\n",
        "\n",
        "  class topic_info(BaseModel):\n",
        "      topic_info: List[topic_info_details]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at generating detailed descriptions for a particular topic using the blog post as context.\n",
        "\n",
        "  Blog Post To Reference Information:\n",
        "  {context}\n",
        "\n",
        "  Topic of interest:\n",
        "  {input_to_llm}\n",
        "\n",
        "  Stick to the above format. Only use the blog post to generate descriptions of the topic. Write the description in atleast 200 words. Stick to the bullet point format for the output.\n",
        "  \"\"\"\n",
        "  pydantic_object=topic_info\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "topic_description = {}\n",
        "\n",
        "for topic in topics:\n",
        "  blog_topics = topic_info_generator(input_to_llm=topic, context=data)\n",
        "  topic_description[topic] = ast.literal_eval(blog_topics[0])['topic_info'][0]['detailed_description'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYH_cAOiCeSS",
        "outputId": "8a44295d-26c6-4c2e-ab91-c2ee188b12ff"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 12425\n",
            "Cost Incurred Input: 0.37275\n",
            "Cost Incurred Output: 0.02574\n",
            "Number of tokens: 12420\n",
            "Cost Incurred Input: 0.3726\n",
            "Cost Incurred Output: 0.00276\n",
            "Number of tokens: 12420\n",
            "Cost Incurred Input: 0.3726\n",
            "Cost Incurred Output: 0.00228\n",
            "Number of tokens: 12422\n",
            "Cost Incurred Input: 0.37266\n",
            "Cost Incurred Output: 0.01848\n",
            "Number of tokens: 12426\n",
            "Cost Incurred Input: 0.37278\n",
            "Cost Incurred Output: 0.056459999999999996\n",
            "Number of tokens: 12427\n",
            "Cost Incurred Input: 0.37281\n",
            "Cost Incurred Output: 0.01512\n",
            "Number of tokens: 12421\n",
            "Cost Incurred Input: 0.37262999999999996\n",
            "Cost Incurred Output: 0.02166\n",
            "Number of tokens: 12422\n",
            "Cost Incurred Input: 0.37266\n",
            "Cost Incurred Output: 0.02124\n",
            "Number of tokens: 12419\n",
            "Cost Incurred Input: 0.37257\n",
            "Cost Incurred Output: 0.01356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwA-WtHvInhT",
        "outputId": "e0dc5d0e-16e6-4c15-c0db-a87da8e05254"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Optimizing your LLM in production': 'Large Language Models (LLMs) such as GPT3/4, Falcon, and LLama are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.',\n",
              " 'Lower Precision': 'Lower Precision: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.',\n",
              " 'Flash Attention': 'Flash Attention is a variation of the attention algorithm that provides a more memory-efficient approach and increased efficiency due to optimized GPU memory utilization.',\n",
              " 'Architectural Innovations': 'Architectural innovations play a crucial role in optimizing large language models (LLMs) for production.',\n",
              " 'Multi-Query-Attention (MQA)': \"Multi-Query-Attention (MQA) is a technique used in Large Language Models (LLMs) to improve computational efficiency and memory consumption during auto-regressive text generation. It was introduced by Noam Shazeer in the paper 'Fast Transformer Decoding: One Write-Head is All You Need'. MQA reduces the number of key-value attention heads to one, which significantly reduces the memory required for storing the key-value cache. Instead of using n_head key-value projection weights, a single head-value projection weight pair is shared across all attention heads. By using a single pair, the key-value vectors remain identical across all attention heads, reducing the memory needed to store them. MQA also improves computational efficiency by reducing the memory bandwidth required for constant reloading of large key-value vectors. This technique has been adopted by many popular LLMs, including Falcon, PaLM, MPT, and BLOOM. MQA allows LLMs to handle longer input sequences and is especially useful for applications like chat, where multiple passes of auto-regressive decoding are required. It provides a significant speed-up and reduces the memory consumption of the key-value cache, making it an essential technique for efficient LLM deployment.\",\n",
              " 'Grouped-Query-Attention (GQA)': 'Grouped-Query-Attention (GQA) is a technique proposed by Ainslie et al. from Google to improve the efficiency and performance of large language models (LLMs) in handling long input sequences.',\n",
              " 'Positional embeddings': 'P',\n",
              " 'The key-value cache': 'The key-value cache is an important component of auto-regressive text generation with large language models (LLMs).',\n",
              " 'Conclusion': 'T'}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Yecr0NJXJI",
        "outputId": "7f1579cb-d621-441b-cfd7-9b538eab02d4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googlesearch import search\n",
        "\n",
        "url_df = pd.DataFrame(columns=['topic','url_list'])\n",
        "\n",
        "for topic in topics:\n",
        "  urls = []\n",
        "  query = topic\n",
        "  for i in search (query,  tld='com', lang='en', tbs='0', safe='off', num=4, start=0, stop=10, domains=None, pause=2.0, tpe='', country='', extra_params=None, user_agent=None):\n",
        "    urls.append(i)\n",
        "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
        "\n",
        "url_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "G9ULnophRW3s",
        "outputId": "8ad35523-0fd4-421e-c036-1305ee3fa927"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n",
            "<ipython-input-105-4f5d2a9fb52f>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  url_df = url_df.append({'topic':topic, 'url_list':urls}, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               topic  \\\n",
              "0  Optimizing your LLM in production   \n",
              "1                    Lower Precision   \n",
              "2                    Flash Attention   \n",
              "3          Architectural Innovations   \n",
              "4        Multi-Query-Attention (MQA)   \n",
              "5      Grouped-Query-Attention (GQA)   \n",
              "6              Positional embeddings   \n",
              "7                The key-value cache   \n",
              "8                         Conclusion   \n",
              "\n",
              "                                            url_list  \n",
              "0  [https://huggingface.co/blog/optimize-llm, htt...  \n",
              "1  [https://www.aeroprecisionusa.com/ar15/lower-r...  \n",
              "2  [https://arxiv.org/abs/2205.14135, https://git...  \n",
              "3  [https://www.aipgh.com/, https://ailincoln.com...  \n",
              "4  [https://blog.fireworks.ai/multi-query-attenti...  \n",
              "5  [https://arxiv.org/abs/2305.13245, https://pap...  \n",
              "6  [https://machinelearningmastery.com/a-gentle-i...  \n",
              "7  [https://medium.com/@joaolages/kv-caching-expl...  \n",
              "8  [https://www.merriam-webster.com/dictionary/co...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34b1142b-efdd-438e-90d4-048dcd9b1393\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>url_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Optimizing your LLM in production</td>\n",
              "      <td>[https://huggingface.co/blog/optimize-llm, htt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lower Precision</td>\n",
              "      <td>[https://www.aeroprecisionusa.com/ar15/lower-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Flash Attention</td>\n",
              "      <td>[https://arxiv.org/abs/2205.14135, https://git...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Architectural Innovations</td>\n",
              "      <td>[https://www.aipgh.com/, https://ailincoln.com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Multi-Query-Attention (MQA)</td>\n",
              "      <td>[https://blog.fireworks.ai/multi-query-attenti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Grouped-Query-Attention (GQA)</td>\n",
              "      <td>[https://arxiv.org/abs/2305.13245, https://pap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Positional embeddings</td>\n",
              "      <td>[https://machinelearningmastery.com/a-gentle-i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The key-value cache</td>\n",
              "      <td>[https://medium.com/@joaolages/kv-caching-expl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Conclusion</td>\n",
              "      <td>[https://www.merriam-webster.com/dictionary/co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34b1142b-efdd-438e-90d4-048dcd9b1393')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-34b1142b-efdd-438e-90d4-048dcd9b1393 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-34b1142b-efdd-438e-90d4-048dcd9b1393');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c30016ec-8d0c-479a-a850-bb0173d8e3fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c30016ec-8d0c-479a-a850-bb0173d8e3fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c30016ec-8d0c-479a-a850-bb0173d8e3fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_173e7930-f5ba-49e5-8a52-c52b37d7aafc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('url_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_173e7930-f5ba-49e5-8a52-c52b37d7aafc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('url_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def url_info_extractor(urls):\n",
        "  data_dict = {}\n",
        "  for url in urls:\n",
        "    loader = WebBaseLoader(url)\n",
        "    try:\n",
        "      data = loader.load()\n",
        "      data_dict[url] = data\n",
        "    except:\n",
        "      print(\"Error\")\n",
        "      data_dict[url] = \"Unable to fetch data\"\n",
        "    else:\n",
        "      print(\"Success!\")\n",
        "  return data_dict\n",
        "\n",
        "url_df['url_with_content'] = url_df['url_list'].apply(url_info_extractor)\n",
        "url_df['url_with_content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhbRFpTBRxBZ",
        "outputId": "d30adf00-3dbf-4fea-d5ae-24a2bd366a0c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n",
            "Success!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    {'https://huggingface.co/blog/optimize-llm': [...\n",
              "1    {'https://www.aeroprecisionusa.com/ar15/lower-...\n",
              "2    {'https://arxiv.org/abs/2205.14135': [page_con...\n",
              "3    {'https://www.aipgh.com/': [page_content='\\n\\n...\n",
              "4    {'https://blog.fireworks.ai/multi-query-attent...\n",
              "5    {'https://arxiv.org/abs/2305.13245': [page_con...\n",
              "6    {'https://machinelearningmastery.com/a-gentle-...\n",
              "7    {'https://medium.com/@joaolages/kv-caching-exp...\n",
              "8    {'https://www.merriam-webster.com/dictionary/c...\n",
              "Name: url_with_content, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(list(url_df['url_with_content'].iloc[0].values())[0][0].page_content, \"gpt-4\")"
      ],
      "metadata": {
        "id": "tybtgz1S75BU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9ec52d-24d8-4944-9325-31d4c2687d48"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11250"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blog_post_inferencer(list(url_df['url_with_content'].iloc[0].values())[0][0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4rLUAacUsrf",
        "outputId": "decd5e1a-8e7e-445f-bbf2-5be13016bbe5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 265\n",
            "Cost Incurred Input: 0.00795\n",
            "Cost Incurred Output: 0.00156\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\\n  \"blog\": [\\n    {\\n      \"topics\": [\"Artificial Intelligence\", \"Machine Learning\"]\\n    }\\n  ]\\n}',\n",
              " 0.009510000000000001,\n",
              " 3.9899497032165527]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def url_post_summarizer(task='skill_extraction', input_to_llm=''):\n",
        "\n",
        "  class url_info_extraction(BaseModel):\n",
        "      blog_post_summary: list = Field(description=\"Summary of the blog post covering all the key points. Topic 1: Description 1, Topic 2: Description 2, ...\")\n",
        "\n",
        "  class url_info(BaseModel):\n",
        "      url_info: List[url_info_extraction]\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  You are an AI assistant tasked at evaluating a blog post. Summarize the blog post highlighting key topics discussed with brief descriptions for each topic:\n",
        "\n",
        "  summary\n",
        "\n",
        "  Blog Post to be evaluated:\n",
        "  {input_to_llm}\n",
        "\n",
        "  Write the summary in atleast 200 words and at the most 500 words. Stick to the format for the output.\n",
        "  \"\"\"\n",
        "  pydantic_object=url_info\n",
        "  pydantic_parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
        "  format_instructions = pydantic_parser.get_format_instructions()\n",
        "  query = prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
        "  )\n",
        "  _input = prompt.format_prompt(query=query)\n",
        "  answer = get_completion(_input.to_string())\n",
        "  return answer\n",
        "\n",
        "url_post_summarizer(input_to_llm=list(url_df['url_with_content'].iloc[0].values())[4][0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcPRAQIxWfzG",
        "outputId": "71b6188c-14eb-4675-ad17-17ebe6032a48"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 3637\n",
            "Cost Incurred Input: 0.10911\n",
            "Cost Incurred Output: 0.11592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\\n  \"url_info\": [\\n    {\\n      \"blog_post_summary\": [\\n        \"Optimizing Large Language Models to Maximize Performance\",\\n        \"Getting the most out of large language models requires the artful application of optimization techniques like prompt engineering, retrieval augmentation, and fine-tuning.\",\\n        \"This guide explores proven methods for maximizing LLM performance.\",\\n        \"In the previous articles, we explored the process of developing effective prompts from scratch.\",\\n        \"However, there are many cases where you inherit existing prompts that have degraded over time or are no longer optimal for current large language models.\",\\n        \"Introduction to the AI Prompt Development Process\",\\n        \"A 15-step methodology for crafting optimized AI prompts that tap into the full potential of AI systems.\",\\n        \"The process aims to maximize relevance, consistency, and quality of outputs.\",\\n        \"Prompt Engineering\",\\n        \"Optimizing large language models (LLMs) for real-world production applications remains one of the most persistent challenges in deploying artificial intelligence systems today.\",\\n        \"Despite incredible advances in model scale and performance on benchmark datasets, tailoring LLMs to reliably solve specialized tasks requires extensive optimization outside of general pretraining.\",\\n        \"This process is difficult for several reasons.\",\\n        \"First, model behavior and failure modes can be highly abstract and difficult to interpret, making it hard to identify exactly where and how optimizations are needed.\",\\n        \"Second, unlike supervised learning, optimizing LLMs is not a linear path.\",\\n        \"Rather, there are two distinct challenges involved - providing sufficient context to the model and programming the desired reasoning behavior.\",\\n        \"Each of these challenges requires a different approach and solution.\",\\n        \"Finally, LLM optimization tends to be an iterative process involving successive rounds of testing, evaluation, and incremental improvement.\",\\n        \"There is no single solution or straightforward methodology.\",\\n        \"Teams must experiment extensively to build an optimization framework tailored to their specific use case.\",\\n        \"However, while difficult, developing a robust optimization strategy enables translating cutting-edge LLMs into performant, reliable AI applications.\",\\n        \"Three major techniques exist for optimizing LLM performance: Prompt optimization, Retrieval-augmented generation, and Fine-tuning.\",\\n        \"These techniques can be combined and applied iteratively to maximize performance on a given task.\",\\n        \"The optimal approach depends on the specific demands of the application.\",\\n        \"Optimizing large language models poses unique challenges due to the immense scale and intricacy of these systems.\",\\n        \"LLMs are trained on extensive datasets encompassing a wide range of topics and styles, making optimization a multi-layered task.\",\\n        \"The diversity of training data allows broad capabilities but also broad potential for unpredictable weaknesses.\",\\n        \"Optimizing prompts and fine-tuning must account for the variability in the model\\'s knowledge.\",\\n        \"Finding the right examples to improve performance in a given niche can be like finding a needle in a haystack.\",\\n        \"The complexity of LLMs\\' architecture, with millions or even billions of parameters, adds to the difficulty of fine-tuning and optimizing these models effectively.\",\\n        \"Their massive scale enables strong general performance but obscures exactly how different prompts and fine-tuning affect model behavior.\",\\n        \"The intricate inner workings of LLMs introduce opacity.\",\\n        \"Users must run rigorous controlled tests to determine optimal prompts and training approaches, rather than relying on intuitions.\",\\n        \"The size and complexity of modern LLMs make optimizing their performance as much art as science.\",\\n        \"It requires experience and diligence to navigate the multitude of factors impacting their capabilities.\",\\n        \"LLMs operate as black boxes, with complex inner representations.\",\\n        \"Failure modes and limitations are often abstract and difficult to interpret.\",\\n        \"It is hard to identify root causes and target optimizations.\",\\n        \"Many possible tweaks across prompts, data, hyperparameters, etc.\",\\n        \"Combinatorial explosions of options to test and evaluate.\",\\n        \"Difficult to isolate the effects of individual changes.\",\\n        \"Benchmark metrics don\\'t always translate to real-world gains.\",\\n        \"Overfitting to benchmarks fails to improve robustness.\",\\n        \"Hard to distinguish true optimization from illusory improvements.\",\\n        \"New model versions released frequently.\",\\n        \"Optimization gains may not transfer between versions.\",\\n        \"The iterative process needs to be restarted with each update.\",\\n        \"The opacity and complexity of large language models create a vast, multidimensional search space for optimizations.\",\\n        \"Progress requires methodically testing changes and quantifying real-world reliability rather than chasing marginal benchmark gains.\",\\n        \"This makes optimizing LLMs uniquely challenging compared to other machine-learning tasks.\",\\n        \"LLM optimization does not follow a simple linear path.\",\\n        \"There are two distinct challenges involved: providing adequate context to the model and programming the desired reasoning behavior.\",\\n        \"Each of these requires a different approach.\",\\n        \"LLMs have a limited context window for new information.\",\\n        \"Retrieval augments long-term memory and relevant knowledge.\",\\n        \"Addresses lack context, but not behavior.\",\\n        \"Fine-tuning updates internal model representations.\",\\n        \"Encodes instructions and reasoning patterns directly into parameters.\",\\n        \"Addresses inconsistent behavior, but no context.\",\\n        \"Context and behavior solutions are complementary.\",\\n        \"Often need both retrieval and fine-tuning to fully optimize.\",\\n        \"Order and priority depend on specific gaps identified.\",\\n        \"There is no universal sequence or precedence of techniques.\",\\n        \"Prompt engineering effectively teaches the LLM new concepts and behaviors.\",\\n        \"However, long prompts strain the model\\'s context window.\",\\n        \"Prompts also cannot efficiently provide external knowledge context to the LLM.\",\\n        \"Retrieval augmentation supplements the LLM\\'s knowledge by retrieving relevant context from an external knowledge source.\",\\n        \"This context is provided alongside the original prompt to inform the model\\'s generation.\",\\n        \"Retrieval augmentation significantly expands the knowledge available to prime the LLM.\",\\n        \"This technique can provide domain-specific vocabulary and facts needed for specialized tasks.\",\\n        \"The context can be updated as the knowledge source evolves.\",\\n        \"However, the retrieval system itself must be tuned to provide useful, relevant information.\",\\n        \"Poor retrievals will not improve and may degrade LLM performance.\",\\n        \"Fine-tuning is a key technique for optimizing large language models for specific use cases.\",\\n        \"Continued Training on Targeted Data.\",\\n        \"Fine-tuning involves continuing an LLM\\'s training with specific, often smaller, datasets tailored to the desired application.\",\\n        \"Transforming General Models into Specialists.\",\\n        \"Fine-tuning takes broad, general-purpose LLMs and transforms them into specialized tools for targeted tasks.\",\\n        \"The same pre-trained LLM can be fine-tuned separately for different applications.\",\\n        \"This allows extracting the maximum value from general models like GPT-3 by customizing them for users\\' specific needs.\",\\n        \"The specialized models can then excel at niche tasks.\",\\n        \"Fine-tuning thus enables LLMs to adapt to a wide range of use cases while retaining their essential capabilities.\",\\n        \"The technique is key for specialized performance.\",\\n        \"Integrate RAG with Fine-Tuned Models.\",\\n        \"Use RAG to supplement the fine-tuned model with additional contextual information.\",\\n        \"Optimize for Balance.\",\\n        \"Ensure a balance between the LLM\\'s general knowledge and its specialized capabilities.\",\\n        \"Continuous Evaluation.\",\\n        \"Regularly assess the LLMâ€™s performance on the target task, using both qualitative and quantitative measures.\",\\n        \"Feedback Loop for Improvement.\",\\n        \"Use the insights from evaluations to further refine the prompts, RAG implementation, and fine-tuning.\",\\n        \"Implement the optimized LLM in a real-world scenario or a testing environment that closely mimics actual use cases.\",\\n        \"Continuously monitor the LLMâ€™s performance in real-world applications, making adjustments as needed based on user feedback and performance data.\",\\n        \"Recognize that LLM optimization is an ongoing process.\",\\n        \"Regularly revisit and update the model with new data, techniques, and insights.\",\\n        \"Developers and researchers can methodically enhance the performance of Large Language Models, tailoring them to specific tasks with increased efficiency and accuracy.\",\\n        \"This approach ensures a comprehensive understanding and application of techniques like prompt engineering, RAG, and fine-tuning, leading to more effective and reliable LLM deployments.\",\\n        \"While the techniques discussed offer proven ways to improve LLM performance, there is no one-size-fits-all approach.\",\\n        \"The optimal strategy depends heavily on the application.\",\\n        \"The optimization strategy that works for one application may not be effective for another due to differing requirements and objectives.\",\\n        \"The optimization process often needs to consider the specific context in which the LLM operates, which can vary greatly from one use case to another.\",\\n        \"The prompts, retrievals, and training data must be tailored to the LLM\\'s intended environment.\",\\n        \"There are best practices, but no \\'magic bullet\\' solutions.\",\\n        \"Ultimately, optimizing LLM performance remains more art than science.\",\\n        \"The methodical testing of different approaches guides users to the right optimizations for their specific needs.\",\\n        \"LLMs are a powerful but temperamental technology.\",\\n        \"Optimizing their performance requires iterative experimentation with prompts, retrievals, and fine-tuning.\",\\n        \"There are no silver bullets, only general guidelines.\",\\n        \"Testing and measurement are critical to determine the right approach for each application.\",\\n        \"When thoughtfully combined, these optimization techniques enable LLMs to fulfill their immense potential.\"\\n      ]\\n    }\\n  ]\\n}',\n",
              " 0.22503,\n",
              " 38.41544771194458]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zmk9x1lOWlwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}